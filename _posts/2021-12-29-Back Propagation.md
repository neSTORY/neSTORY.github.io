## Linear Regeression

연속적인 값을 갖는 경우 선형 회귀를 통해 값을 예측할 수 있다.

가설함수 hypothesis를 y = Wx + b로 정의할 수 있고 이 함수를 통해 나온 예측값 yhat을 실제 값인 y와의 차이를 줄여가는 방향으로 이상적인 회귀식을 구할 수 있다.

차이를 줄이는 방법은 cost function을 미분하여 기울기값을 줄여서 cost(error)를 최소화 하는 것이다. 

W, b에 대한 cost 함수는 다음 그림과 같다.

![image-20211216165259690](md-images/image-20211216165259690.png)

H(xi)는 x에 관한 가설함수 wx+b이고 y는 실제 값이다. 

여기서 제곱을 해주는 이유는 오차는 양수, 음수가 있으므로 오차들을 합할 때 음수 때문에 cost가 오히려 줄어들 수 있기 때문에 제곱을 한 값을 더 해주는 것이다.

m으로 나누는 것이 아닌 2m으로 나눈 이유는 미분할 때 제곱 때문에 2가 곱해지기 때문에 상쇄시키려고 하는 것이다.

이 cost를 w에 관하여 편미분하면 다음 그림과 같다.

![image-20211216165735469](md-images/image-20211216165735469.png)

이렇게 w에 관해 편미분한 값을 원래 W에 빼서 weight를 업데이트 해주는 것이다.







## Back Propagation

마빈민스키 아저씨가 1969년에 mlp(multi layer perceptron)의 수 많은 파라미터 값을 구할 순 없다. 라고 말한다. 그런데 후에 어떤 아저씨가 74,82년에 back propagation이라는 논문을 발표하고  역방향으로 cost function을 미분해서 error를 줄일 수 있다고 말하지만 마빈 민스키 아저씨는 시큰둥 했다. 



그런데 86년 힌튼 교수가 다시 back propagation 논문을 발표하여 불가능하지 않다는 것을 증명했다.



이 역전파의 개념은 복합함수 f(g(x)) 형태의 cost function을 역방향으로 미분하여 기울기를 줄여가며 error를 줄일 수 있다는 것이다.